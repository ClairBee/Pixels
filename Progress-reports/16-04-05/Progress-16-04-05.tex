\documentclass[10pt,fleqn]{article}
\usepackage{/home/clair/Documents/mystyle}

%----------------------------------------------------------------------
% reformat section headers to be smaller \& left-aligned
\titleformat{\section}
	{\normalfont\bfseries}
	{\thesection}{1em}{}
	
\titleformat{\subsection}
	{\normalfont\bfseries}
	{\llap{\parbox{1cm}{\thesubsection}}}{0em}{}
	
%----------------------------------------------------------------------
% SPECIFY BIBLIOGRAPHY FILE & FIELDS TO EXCLUDE
%\addbibresource{bibfile.bib}
%\AtEveryBibitem{\clearfield{url}}
%\AtEveryBibitem{\clearfield{doi}}
%\AtEveryBibitem{\clearfield{isbn}}
%\AtEveryBibitem{\clearfield{issn}}

%----------------------------------------------------------------------

%======================================================================

\usepackage{fancyhdr}

\fancyhf{}
\fancyhead[R]{\textbf{7-Apr-2016}}
\pagestyle{fancy}

\addtolength{\topmargin}{-0.5cm}
\addtolength{\textheight}{1.4cm}

\renewcommand{\headrulewidth}{0pt}

\begin{document}

\subsection*{Since last week}

\begin{itemize}

\item Fitted linear models to white, grey \& black images, allowing polynomial terms of parameters. Residuals have much lower SD/MAD than previously (esp. in white channel) but still show signs of systematic variation - dip in values at edges of panels and possible diagonal `wave' effect (possible $1/f$ noise?)?
 
\item Started to consider movement of pixels between states, beginning with pixels that settle at 65535 pixels for a full day's acquisitions (eg. pixelwise mean value == 65535, pixelwise SD = 0). Need to consider how to account for whole-panel offset when comparing between acquisitions.

\item Implemented flat field correction, but some panelwise variation still remains - linear gradient per panel seems to help, reducing NMAD to 55 (starting NMAD for white channel is ~1800)

\item Using two approaches to thresholding after fitting a distribution to the residuals: either cutting at a quantile (maybe using a variant of the boxplot outlier calculation: $Q_{0.001} - 1.5 \cdot IQR$, $Q_{0.999} + 1.5 \cdot IQR$) or using quartiles of the \textbf{Johnson distribution}.

\item Loess smoothing is particularly ineffective at edges of panels, failing to identify columns of bad pixels particularly at LHS. Anything involving differencing/smoothing against neighbours will likely suffer the same effect.

\item Simple parametric model (even with only circular spot or only panels) picks up large proportion of bad pixels missed by official map.

\item Compared robust \& non-robust model fitting in areas of high damage (using top left-hand panel as test case), also cropping edge of panel (since this is more likely to show damage). Robust model fitting consistently better (lower SD, and mean value less affected by outliers) but cropping makes no real difference.

\item Assessment of model fit is being thrown by large numbers of low-valued pixels around edges of panel. May be useful to crop these (say, remove 50px from each edge), fit thresholds to central 'healthy' portion (using whichever method), then apply those thresholds to the whole image again.

\item Visible line of dead pixels in images from 16-03-14, panel U4.
\end{itemize}

\subsection*{Current questions/considerations}
\begin{itemize}

\item In terms of thresholding, would propose using a parametric model to reduce systematic variability (circular spot \& panelwise regression), then fitting a Johnson distribution to the residuals. Quantiles of Johnson distribution can be used to as `sickness' score to assess how far a pixel is from its expected value, and to set a threshold. \textbf{Some danger of circular reasoning:} must make sure we don't identify regions of bad pixels simply because the model doesn't fit the data well enough!

\begin{itemize}
\item In dark images, distribution is v narrow, picking up large numbers of `bad' pixels on both sides of main distribution; however, same threshold in grey and white images picks up a relatively small number of points.

\item If not Johnson distribution, could use MAD to get a `sickness' score for each pixel as discussed previously
\end{itemize}

\item What size of image defect are we looking for? (ie. do we want to ignore spots on screen etc, or should those be picked up as error areas as well?) 

\item At what point is the model sufficient? Four potential candidates so far:

\begin{itemize}
\item flat field correction (plus panelwise/spot correction?)
\item thresholding as given in manual
\item parametric noise model fitted to white/grey images (dome \& panels at least)
\item direct loess smoothing along columns \& rows, identify any `outlier' points that don't belong to usual pattern. (likely to be confounded by panel edges to some degree) 
\end{itemize}

\end{itemize}


\subsection*{Next steps}

\begin{itemize}

\item Finish tuning parametric model

\begin{itemize}
\item Try fitting per-panel parametric model across distance from top-left corner of panel, rather than by x and y coordinates?
\item May need curved fit across Y axis of panel, not sure about X-axis

\end{itemize}

\item Compare results with each given approach (both in terms of residuals \& in terms of `official' bad pixels identified)

\item When fitting linear panels, break vertically into 4 as well as horizontally into 16. Gradients are not constant and there appears to be another offset at this point, albeit a smaller one.

\item Per-column thresholding with raw residuals, not taking abs. value to get amplitude

\item Go back and look at daily SD again - any points with high daily SD are of interest, as are any with high mean value


\end{itemize}


\end{document}