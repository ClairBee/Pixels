\documentclass[10pt,fleqn]{article}
\usepackage{/home/clair/Documents/mystyle}

%----------------------------------------------------------------------
% reformat section headers to be smaller \& left-aligned
\titleformat{\section}
	{\normalfont\bfseries}
	{\thesection}{1em}{}
	
\titleformat{\subsection}
	{\normalfont\bfseries}
	{\llap{\parbox{1cm}{\thesubsection}}}{0em}{}
	
%----------------------------------------------------------------------
% SPECIFY BIBLIOGRAPHY FILE & FIELDS TO EXCLUDE
%\addbibresource{bibfile.bib}
%\AtEveryBibitem{\clearfield{url}}
%\AtEveryBibitem{\clearfield{doi}}
%\AtEveryBibitem{\clearfield{isbn}}
%\AtEveryBibitem{\clearfield{issn}}

%----------------------------------------------------------------------
% define code format
\lstnewenvironment{code}[1][R]{			% default language is R
	\lstset{language = #1,
        		columns=fixed,
    			tabsize = 4,
    			breakatwhitespace = true,
			showspaces = false,
    			xleftmargin = .75cm,
    			lineskip = {0pt},
			showstringspaces = false,
			extendedchars = true
    }}
    {}
    
%======================================================================

\usepackage{fancyhdr}

\fancyhf{}
\fancyhead[R]{\textbf{03-Mar-2016}}
\pagestyle{fancy}

\addtolength{\topmargin}{-0.5cm}
\addtolength{\textheight}{1.4cm}

\renewcommand{\headrulewidth}{0pt}

\begin{document}

\subsection*{Since last week}

\begin{itemize}

\item
Brief literature review carried out. Several broadly different approaches to identification of bad pixels found:

\begin{itemize}

\item Absolute cutoff based on mean/SD value (Thresholding approach varies: based on a given \% of mean/median value, or on difference between $Q_{99.9}$ (say) and modal value)

\item Max/min filtering (if pixel is more extreme than all neighbours, likely to be defective) \textit{(Produced c. 80k pixels that were local min/max of $9 \times 9$ neighbours: particularly at panel edges. Need to come up with something more sophisticated)}

\item Convolution with neighbouring pixels, based on linear kernels: if smallest convolution is large, we have an isolated pixel

\item Principal component analysis \textit{(Not sure if this is appropriate here: some papers treat successive frames as independent observations of pixel value, but can't claim independence)} 

\end{itemize}

\item
Recreated (partially) the thresholding approach used in the manual to classify dead, hot, noisy, dim and bright pixels in the first and last acquisition sets. Different channels are more prone to different types of `bad' pixels, and spatial distribution of each seems to differ.

\item
Also briefly compared effect of adjusting threshold. No obvious better candidate in terms of absolute cutoff (unsurprisingly) but approach looking at difference between modal value and quartile may be more useful.
\end{itemize}

\subsection*{Current questions/considerations}
\begin{itemize}

\item
Need to understand how local/glocal thresholding works in the manual. Seems to be run on an adjusted image, but exact details aren't clear, so would like to discuss further with Jay (unless this has already been clarified?) \textit{(Tried to run on one channel's data but picked up 800k pixels either side...)}

\item
Not picking up many lines (only at top-left corner of white/grey channel). How concerned should we be about this?

\item Does purpose of image affect how conservative we want to be in our assessment of `too many' bad pixels? eg. when looking for defects, a bad pixel will signal a false alarm - but is a single isolated pixel enough to signal this? %Need to check whether these methods will work for a high density of bad pixels, and whether they will be able to identify vertical lines of bad pixels (eg. may need to convolve without vertical-line kernel)

\item Need to decide on a hierarchy of problems: is a noisy pixel worse than one which is consistently too bright or too dim? Or should we treat these as entirely separate (bad value vs too variable, for example)

\item Intra-panel dependency: is this something I should be looking at, or is Audrey tacklng this already?
\end{itemize}




\subsection*{Next steps}

\begin{itemize}

\item
Use thresholds based on difference between modal values and quantiles to classify bad pixels of various types as in [Pinto2012]

\item
Implement convolution-based approach to identifying locally non-uniform points as in [Zhang2002]: again, likely to be confounded by panel edges, but may be useful with appropriate threshold setting.

\item
Consider plotting evolution of pixel population at each time stage (maybe as a tree diagram? Or is there a better way to visualise this?)

\item
Look at SD across daily acquisitions - how high is the SD of a `noisy' pixel? Is there any pattern in the variability? 
%In particular, plot daily time series of pixels with high daily SD - these may be showing short-term 'flickering'/'blipping' behaviour. (Those investigated so far seem fairly constant in the short term)

%For all `bad' pixels identified, plot daily pixelwise mean for full sequence of measurements

%\item
%For each panel, look at pixelwise mean \& SD - should be a relationship between the two, which will differ across the panels (intra-panel dependency)

\end{itemize}

%\newpage
%\printbibliography
\end{document}